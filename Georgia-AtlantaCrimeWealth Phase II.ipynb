{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ4Xt1uf1LT7"
   },
   "source": [
    "# Final Project Phase 2 Summary\n",
    "This Jupyter Notebook (.ipynb) will serve as the skeleton file for your submission for Phase 2 of the Final Project. Answer all statements addressed below as specified in the instructions for the project, covering all necessary details. Please be clear and concise in your answers. Each response should be at most 3 sentences. Good luck! <br><br>\n",
    "\n",
    "Note: To edit a Markdown cell, double-click on its text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kendall Roberts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp7Pm-Suh3d"
   },
   "source": [
    "# Data Sources\n",
    "Include sources (as links) to your datasets. Add any additional data sources if needed. Clearly indicate if a data source is different from one submitted in your Phase I, as we will check that it satisfies the requirements.\n",
    "*   Downloaded Dataset Source:\n",
    "https://www.atlantapd.org/home/showpublisheddocument/3051/637141450644130000\n",
    "\n",
    "#This starts a zip download that contains the csv file: COBRA-2009-2019.csv\n",
    "Website: https://www.atlantapd.org/i-want-to/crime-data-downloads\n",
    "\n",
    "*   Web API Collection Source:\n",
    "This is different than the API one I submitted. I swapped the old source out since the old API had limited imformation that could be provided and the amount of requests I could make was very limited.\n",
    "https://crime-data-explorer.fr.cloud.gov/pages/docApi\n",
    "\n",
    "\n",
    "*   Web HTML Collection Source:\n",
    "This is different than the html one I submitted. I swapped the old source out since the old HTML had limited imformation that could be used.\n",
    "https://www.zipdatamaps.com/economics/income/agi/metro/wealthiest-zipcodes-in-metro-atlanta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mRjxZDbE1tj"
   },
   "source": [
    "## Downloaded Dataset Requirement\n",
    "\n",
    "Fill in the predefined functions with your data scraping/parsing code. You may modify/rename each function as you seem fit, but you must provide at least 3 separate functions that clean each of your required datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0p5xxmqzFGrO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spain\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (3,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ReportDate   OccurDate  OccurTime  PoliceBeat  \\\n",
      "Report Number                                                  \n",
      "160442700       2/13/2016    1/7/1916       1215       412.0   \n",
      "160902737       3/30/2016   3/29/1916       2300       507.0   \n",
      "160951996        4/4/2016    4/2/1916       1700       211.0   \n",
      "161460989       5/25/2016   5/15/1916       2000       206.0   \n",
      "161592043        6/7/2016   5/30/1916       1400       412.0   \n",
      "...                   ...         ...        ...         ...   \n",
      "193652089      12/31/2019  12/31/2019       2030       105.0   \n",
      "193650336      12/31/2019  12/31/2019        432       206.0   \n",
      "193650603      12/31/2019  12/31/2019        920       404.0   \n",
      "193651760      12/31/2019  12/31/2019       1853       404.0   \n",
      "193652591      12/31/2019  12/31/2019       2045       406.0   \n",
      "\n",
      "                         StreetAddress ShiftOccurence            CrimeType  \\\n",
      "Report Number                                                                \n",
      "160442700              710 JEWEL CT SW        Evening   BURGLARY-RESIDENCE   \n",
      "160902737             180 WALKER ST SW        Evening           AUTO THEFT   \n",
      "160951996          2175 PIEDMONT RD NE        Evening      BURGLARY-NONRES   \n",
      "161460989      102 W PACES FERRY RD NW            Day  LARCENY-NON VEHICLE   \n",
      "161592043            4666 EDWINA LN SW        Evening  LARCENY-NON VEHICLE   \n",
      "...                                ...            ...                  ...   \n",
      "193652089            1385 SHARON ST NW        Morning          AGG ASSAULT   \n",
      "193650336              262 PHARR RD NE        Morning          AGG ASSAULT   \n",
      "193650603           689 CASCADE AVE SW            Day          AGG ASSAULT   \n",
      "193651760           763 CASCADE AVE SW        Evening          AGG ASSAULT   \n",
      "193652591             237 PEYTON PL SW        Evening          AGG ASSAULT   \n",
      "\n",
      "               UCR # IBR Code NPU  Latitude  Longitude  ReportMonth  \\\n",
      "Report Number                                                         \n",
      "160442700        511     2202   Q  33.72583  -84.55050            2   \n",
      "160902737        710    2404A   M  33.74964  -84.40132            3   \n",
      "160951996        522     2205   F  33.81716  -84.36632            4   \n",
      "161460989        670     2308   B  33.84131  -84.38427            5   \n",
      "161592043        670     2308   P  33.70309  -84.54044            6   \n",
      "...              ...      ...  ..       ...        ...          ...   \n",
      "193652089        420    1315K   K  33.75486  -84.43287           12   \n",
      "193650336        410     1314   B  33.83732  -84.37860           12   \n",
      "193650603        410     1314   T  33.73636  -84.43680           12   \n",
      "193651760        410     1314   T  33.73483  -84.43750           12   \n",
      "193652591        410     1314   S  33.72518  -84.45013           12   \n",
      "\n",
      "               ReportDay  ReportYear  OccurMonth  OccurDay  OccurYear  \\\n",
      "Report Number                                                           \n",
      "160442700             13        2016           1         7       1916   \n",
      "160902737             30        2016           3        29       1916   \n",
      "160951996              4        2016           4         2       1916   \n",
      "161460989             25        2016           5        15       1916   \n",
      "161592043              7        2016           5        30       1916   \n",
      "...                  ...         ...         ...       ...        ...   \n",
      "193652089             31        2019          12        31       2019   \n",
      "193650336             31        2019          12        31       2019   \n",
      "193650603             31        2019          12        31       2019   \n",
      "193651760             31        2019          12        31       2019   \n",
      "193652591             31        2019          12        31       2019   \n",
      "\n",
      "               OccurTimeStandard AM/PM  \n",
      "Report Number                           \n",
      "160442700                     15    PM  \n",
      "160902737                   1100    PM  \n",
      "160951996                    500    PM  \n",
      "161460989                    800    PM  \n",
      "161592043                    200    PM  \n",
      "...                          ...   ...  \n",
      "193652089                    830    PM  \n",
      "193650336                    432    AM  \n",
      "193650603                    920    AM  \n",
      "193651760                    653    PM  \n",
      "193652591                    845    PM  \n",
      "\n",
      "[342578 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "def data_parser():\n",
    "    \n",
    "    cobra_df = pd.read_csv(\"COBRA-2009-2019.csv\", delimiter = \",\", index_col = 0)\n",
    "    \n",
    "    cobra_df.drop([\"Location Type\",\"Apartment Number\",\"Neighborhood\",\"Apartment Office Prefix\",\"Possible Time\",\"Possible Date\"],axis = 1, inplace=True)\n",
    "    \n",
    "    #cobra_df.isnull().sum()\n",
    "    cobra_df.dropna(inplace = True)\n",
    "    \n",
    "    cobra_df.rename(columns = {\"UCR Literal\":\"CrimeType\",\"Beat\":\"PoliceBeat\",\"Location\":\"StreetAddress\"},inplace = True)\n",
    "    cobra_df.rename(columns = {\"Report Date\":\"ReportDate\",\"Occur Date\":\"OccurDate\",\"Occur Time\":\"OccurTime\",\"Shift Occurence\":\"ShiftOccurence\"},inplace = True)\n",
    "    \n",
    "    cobra_df[[\"ReportMonth\",\"ReportDay\",\"ReportYear\"]] = cobra_df[\"ReportDate\"].str.split(\"/\", expand = True).astype(int)\n",
    "    cobra_df[[\"OccurMonth\",\"OccurDay\",\"OccurYear\"]] = cobra_df[\"OccurDate\"].str.split(\"/\", expand = True).astype(int)\n",
    "    \n",
    "    cobra_df[\"ShiftOccurence\"] = cobra_df[\"ShiftOccurence\"].map({\"Evening Watch\":\"Evening\",\"Day Watch\":\"Day\",\"Morning Watch\":\"Morning\"})\n",
    "    \n",
    "    cobra_df = cobra_df[~pd.to_numeric(cobra_df[\"OccurTime\"], errors='coerce').isna()]\n",
    "    \n",
    "    cobra_df[\"OccurTime\"] = cobra_df[\"OccurTime\"].astype(int)\n",
    "    \n",
    "    cobra_df[\"OccurTimeStandard\"] = (cobra_df[\"OccurTime\"] < 1200) | (cobra_df[\"OccurTime\"] == 2400)\n",
    "    \n",
    "    cobra_df[\"AM/PM\"] = np.where(cobra_df[\"OccurTimeStandard\"] == True, \"AM\",\"PM\")\n",
    "    \n",
    "    cobra_df[\"OccurTimeStandard\"] = np.where(cobra_df[\"OccurTime\"] <= 1200, cobra_df[\"OccurTime\"], cobra_df[\"OccurTime\"] - 1200)\n",
    "    \n",
    "    cobra_df.sort_values(by=[\"OccurYear\", \"OccurMonth\", \"OccurDay\"], ascending=True, inplace=True)\n",
    "    \n",
    "    print(cobra_df)\n",
    "    \n",
    "    cobra_df.to_csv(\"Cleaned_COBRA-2009-2019.csv\", index = True)\n",
    "\n",
    "############ Function Call ############\n",
    "data_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloaded Dataset Explanation\n",
    "1. Imported the data and saved it as cobra_df\n",
    "\n",
    "2. Since Apartment Office Prefix, Apartment Number, Location Type, and Neighboorhood had a lot of missing data (332,820),   (274,640), (9,216), (123,630) missing values respectively, I removed these columns since this amount of missing data makes these columns useless. Used cobra_df.isnull().sum() to find out these numbers. Also got rid of Possible Time and Possible Date since these columns seemed redundant or useless.\n",
    "\n",
    "3. Dropped the rows which contained null data since it would make further operations difficult and there were relatively few of them so not a huge impact on the data sample.\n",
    "\n",
    "4. Renamed columns: UCR Literal, Beat, Location to CrimeType, PoliceBeat, StreetAddress for more clear column names. Also renamed the column names with spaces to not have any spaces for easier use.\n",
    "\n",
    "5. In the ShiftOccurence column used .map() to change Evening Watch to Evening, Morning Watch to Morning, Day Watch to Day\n",
    "\n",
    "6. Checked all the values in the OccurTime to see if they were numeric values and if the value was not numeric then that row containing that value was removed.\n",
    "\n",
    "7. Converted the OccurTime column values to int type\n",
    "\n",
    "8. Creates a bool value column called OccurTimeStandard that has True for the value if the value in the OccurTime column is less than 1200 or equal to 2400.\n",
    "\n",
    "9. Creates a column called AM/PM that uses np.where to put AM if the value in the OccurTime column is True and PM if the value is False.\n",
    "\n",
    "10. Changes the OccurTimeStandard column back to the OccurTime column values with a cap of 1200 for the occur time to more closely follow the standard time format.\n",
    "\n",
    "11. Sorted the cobra_df values by OccurYear, OccurMonth, then OccurDay\n",
    "\n",
    "12. Print out cobra_df then write it to a csv file called Cleaned_COBRA-2009-2019.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "794L4vGXFdYw"
   },
   "source": [
    "## Web Collection Requirement \\#1API Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vXwpJObDFiWM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Actual  Agency  Cleared  DataYear             Offense State  \\\n",
      "0         1   10000        1    2009.0  Aggravated-Assault    GA   \n",
      "1        17   10100       14    2009.0  Aggravated-Assault    GA   \n",
      "2        13   20100        4    2009.0  Aggravated-Assault    GA   \n",
      "3         1   20200        3    2009.0  Aggravated-Assault    GA   \n",
      "4         4   30000        2    2009.0  Aggravated-Assault    GA   \n",
      "..      ...     ...      ...       ...                 ...   ...   \n",
      "135       0   70200        0    2009.0             Robbery    GA   \n",
      "136       2   70300        0    2009.0             Robbery    GA   \n",
      "137      29   80000        3    2009.0             Robbery    GA   \n",
      "138      17   80100        9    2009.0             Robbery    GA   \n",
      "139       3   80200        1    2009.0             Robbery    GA   \n",
      "\n",
      "     ClearedToActualRatio  ClearedPercentage  \n",
      "0                1.000000         100.000000  \n",
      "1                0.823529          82.352941  \n",
      "2                0.307692          30.769231  \n",
      "3                3.000000         300.000000  \n",
      "4                0.500000          50.000000  \n",
      "..                    ...                ...  \n",
      "135              0.000000           0.000000  \n",
      "136              0.000000           0.000000  \n",
      "137              0.103448          10.344828  \n",
      "138              0.529412          52.941176  \n",
      "139              0.333333          33.333333  \n",
      "\n",
      "[140 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "def API_parser():\n",
    "    \n",
    "    my_key = \"Advdd3veLeje4QGRZytKos2jh9B5RsIWwCX1hcLa\"\n",
    "    \n",
    "    crime_key_list = [\"aggravated-assault\", \"burglary\", \"larceny\", \"motor-vehicle-theft\", \"homicide\", \"rape\", \"robbery\"]\n",
    "    \n",
    "    crime_list = []\n",
    "    \n",
    "    column_names=[\"Actual\",\"Cleared\",\"DataYear\",\"Offence\",\"Agency\",\"State\"]\n",
    "    crime_df = pd.DataFrame()\n",
    "    \n",
    "    for crime in crime_key_list:\n",
    "        r = requests.get(\"https://api.usa.gov/crime/fbi/sapi/api/summarized/state/GA/\" + crime + \"/2009/2019?API_KEY=\" + my_key)\n",
    "        data = r.json()\n",
    "        crime_list.append(data[\"results\"])\n",
    "    \n",
    "    temp_list = []\n",
    "    for list in crime_list:\n",
    "        temp_list = temp_list + list\n",
    "    \n",
    "    for dict in temp_list:\n",
    "        crime_df = crime_df.append({\"Actual\":dict[\"actual\"],\"Cleared\":dict[\"cleared\"],\"DataYear\":dict[\"data_year\"],\n",
    "                                    \"Offense\":dict[\"offense\"],\"Agency\":dict[\"ori\"],\"State\":dict[\"state_abbr\"]}, ignore_index = True)\n",
    "        \n",
    "    crime_df[\"Cleared\"] = crime_df[\"Cleared\"].astype(int)\n",
    "    crime_df[\"Actual\"] = crime_df[\"Actual\"].astype(int)\n",
    "    \n",
    "    crime_df[\"ClearedToActualRatio\"] = crime_df[\"Cleared\"] / crime_df[\"Actual\"]\n",
    "    \n",
    "    crime_df[\"ClearedToActualRatio\"] = crime_df[\"ClearedToActualRatio\"].fillna(0)\n",
    "    \n",
    "    crime_df[\"ClearedPercentage\"] = crime_df[\"ClearedToActualRatio\"] * 100\n",
    "    \n",
    "    crime_df[\"Agency\"] = crime_df[\"Agency\"].str.replace(r\"GA\",\"\").astype(int)\n",
    "    \n",
    "    crime_df[\"Offense\"] = crime_df[\"Offense\"].str.title()\n",
    "    \n",
    "    print(crime_df)\n",
    "    \n",
    "    crime_df.to_csv(\"Cleaned_GA_Crime_Data_2009_2019.csv\", index = True)\n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "API_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Collection Requirement #1 API Dataset Explaination\n",
    "1. Access the API using my key and iterate through the different crime keywords using crime_key_list and converting it to a dictionary.\n",
    "\n",
    "2. Appends the dictionaries to a list called crime_list then iterates through crime_list to create temp_list which converts the data from a list of lists of dicts into only a list of dicts.\n",
    "\n",
    "3. Iterates through temp_list and appends the dicts to a pandas dataframe called crime_df with columns: Actual, Cleared, DataYear, Offense, renamed ori to Agency to be more descriptive, and renamed state_abbr to just State. Excluded Data Range column since all values were none. \n",
    "\n",
    "4. Created a column called ClearedToActualRatio which is the ratio of cleared cases to actual cases\n",
    "\n",
    "5. Created a column called ClearedPercentage from the ClearedToActualRatio column that represents the percentage of crimes cleared for that specific crime, agency, and year\n",
    "\n",
    "6. Gets rid of the GA infront of the values of the Agency column values since that is not need since all the data is from Georgia and converts all the values to int.\n",
    "\n",
    "7. Converts the values in the Offense Column to uppercase\n",
    "\n",
    "8. Writes the crime_df to a csv file called Cleaned_GA_Crime_Data_2009_2019.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDD6sMsCXRxc"
   },
   "source": [
    "## Web Collection Requirement \\#2 HTML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HAkUOqMgXQJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank  Zipcode         ZipName   County  AdjustedGrossIncome  \\\n",
      "0      1    30327         Atlanta   Fulton               563160   \n",
      "1      2    30305         Atlanta   Fulton               340890   \n",
      "2      3    30342         Atlanta   Fulton               205630   \n",
      "3      4    30306         Atlanta   Fulton               204020   \n",
      "4      5    30326         Atlanta   Fulton               200640   \n",
      "5      6    30319         Atlanta   DeKalb               198810   \n",
      "6      7    30309         Atlanta   Fulton               178530   \n",
      "7      8    30328         Atlanta   Fulton               160570   \n",
      "8      9    30068        Marietta     Cobb               158120   \n",
      "9     10    30097          Duluth   Fulton               158110   \n",
      "10    11    30307         Atlanta   DeKalb               154170   \n",
      "11    12    30075         Roswell   Fulton               152860   \n",
      "12    13    30004      Alpharetta   Fulton               151620   \n",
      "13    14    30338         Atlanta   DeKalb               145840   \n",
      "14    15    30009      Alpharetta   Fulton               142030   \n",
      "15    16    30022      Alpharetta   Fulton               139980   \n",
      "16    17    30041         Cumming  Forsyth               138310   \n",
      "17    18    30005      Alpharetta   Fulton               138290   \n",
      "18    19    30030         Decatur   DeKalb               134020   \n",
      "19    20    30339         Atlanta     Cobb               133460   \n",
      "20    21    30363         Atlanta   Fulton               125780   \n",
      "21    22    30269  Peachtree City  Fayette               120480   \n",
      "22    23    30308         Atlanta   Fulton               119990   \n",
      "23    24    30350         Atlanta   Fulton               118410   \n",
      "24    25    30336         Atlanta   Fulton               117720   \n",
      "\n",
      "    MedianHouseholdIncome  \n",
      "0                  154738  \n",
      "1                  101718  \n",
      "2                   84458  \n",
      "3                  104934  \n",
      "4                   82225  \n",
      "5                  100679  \n",
      "6                   88691  \n",
      "7                   83860  \n",
      "8                  117475  \n",
      "9                  101119  \n",
      "10                  98986  \n",
      "11                 116923  \n",
      "12                 111925  \n",
      "13                 101982  \n",
      "14                  91811  \n",
      "15                 105988  \n",
      "16                 107284  \n",
      "17                 127764  \n",
      "18                  81331  \n",
      "19                  79883  \n",
      "20                  59591  \n",
      "21                  95837  \n",
      "22                  60370  \n",
      "23                  64722  \n",
      "24                  32501  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-602d4c32ebbf>:22: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  wealth_df[\"AdjustedGrossIncome\"] = wealth_df[\"AdjustedGrossIncome\"].str.replace(r\"$\",\"\").astype(int)\n",
      "<ipython-input-14-602d4c32ebbf>:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  wealth_df[\"MedianHouseholdIncome\"] = wealth_df[\"MedianHouseholdIncome\"].str.replace(r\"$\",\"\").astype(int)\n"
     ]
    }
   ],
   "source": [
    "def web_html_parser():\n",
    "    r = requests.get(\"https://www.zipdatamaps.com/economics/income/agi/metro/wealthiest-zipcodes-in-metro-atlanta\")\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    wealth_chart_tag = soup.find(\"table\",{\"class\":\"table table-striped table-bordered table-hover table-condensed\"}) \n",
    "    tags = wealth_chart_tag.find_all(\"tr\")\n",
    "    \n",
    "    column_names=[\"Rank\",\"Zipcode\",\"ZipName\",\"County\",\"AdjustedGrossIncome\",\"MedianHouseholdIncome\"]\n",
    "    wealth_df = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    for tag in tags[2:]:\n",
    "        data_tags = tag.find_all(\"td\")\n",
    "        \n",
    "        row_list = [tag.text for tag in data_tags if tag.text != \"\" or tag != None]\n",
    "        \n",
    "        if len(row_list) == 10:\n",
    "            wealth_df = wealth_df.append({\"Rank\":row_list[0], \"Zipcode\":row_list[5], \"ZipName\":row_list[6], \"County\":row_list[7], \"AdjustedGrossIncome\":row_list[8], \"MedianHouseholdIncome\":row_list[9]}, ignore_index = True)\n",
    "    \n",
    "    wealth_df[\"Rank\"] = wealth_df[\"Rank\"].astype(int)\n",
    "    wealth_df[\"Zipcode\"] = wealth_df[\"Zipcode\"].astype(int)\n",
    "    \n",
    "    wealth_df[\"AdjustedGrossIncome\"] = wealth_df[\"AdjustedGrossIncome\"].str.replace(r\"$\",\"\").astype(int)\n",
    "    wealth_df[\"MedianHouseholdIncome\"] = wealth_df[\"MedianHouseholdIncome\"].str.replace(r\"$\",\"\").astype(int)\n",
    "    \n",
    "    print(wealth_df)\n",
    "    \n",
    "    wealth_df.to_csv(\"Cleaned_Zipcode_Wealth_Data.csv\", index = True)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "############ Function Call ############\n",
    "web_html_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Collection Requirement #2 HTML Dataset Explaination\n",
    "1. Send a request to the Web page I am interested in.Then use bs4 html.parser to obtain a soup object\n",
    "\n",
    "2. Created a tag called wealth_chart_tag that uses .find() contains the table tag, then uses .find_all() to find all \"tr\" tags\n",
    "\n",
    "3. Created a dataframe called wealth_df with columns Rank, Zipcode, ZipName, County, AdjustedGrossIncome, MedianHouseholdIncome.Then iterates through all the tr tags excluding the first two which were empty\n",
    "\n",
    "4. Used find_all() to find all td tags with the current tr tag and stored it in a value data_tags\n",
    "\n",
    "5. Created a list comprehension called row_list that converts the tags in data_tags to text if they are not None or \"\"\n",
    "\n",
    "6. Adds a row to wealth_df by using .append() mapping the column name to the index in row_list only if that row_list is of the correct length.\n",
    "\n",
    "7. Converted the Rank and Zipcode columns to int type\n",
    "\n",
    "8. For the AdjustedGrossIncome and MedianHouseholdIncome columns removed the dollar sign infront of the numbers and then converts the columns to a int type\n",
    "\n",
    "9. Print out wealth_df then write it to a csv file called Cleaned_Zipcode_Wealth_Data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezovwa1tp0we"
   },
   "source": [
    "## Additional Dataset Parsing/Cleaning Functions\n",
    "\n",
    "Write any supplemental (optional) functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4-s72RNuKLR"
   },
   "outputs": [],
   "source": [
    "def extra_source1():\n",
    "    pass\n",
    "\n",
    "    \n",
    "############ Function Call ############\n",
    "extra_source1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB3qXt_XuY7b"
   },
   "outputs": [],
   "source": [
    "# Define further extra source functions as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttEYrm9US5s"
   },
   "source": [
    "# Inconsistencies\n",
    "For each inconsistency (NaN, null, duplicate values, empty strings, etc.) you discover in your datasets, write at least 2 sentences stating the significance, how you identified it, and how you handled it.\n",
    "\n",
    "1. For the Downloaded Data set in multiple columns there are many null and empty data values. For the columns that had way too many null data values or empty data values for the data to be useful, I dropped those columns. For the columns that had relatively few null or empty values I dropped the row the value was contained in. I used cobra_df.isnull().sum() to count the number of null values in each column so I could decide whether to drop the column or the row. I used cobra_df.dropna(inplace = True) to drop missing values.\n",
    "\n",
    "\n",
    "2. For the Downloaded Data set the OccurTime column contained mixed data type; mostly possible ints values with a incorrectly entered value with a letter \"T\" instead of a number. \n",
    "\n",
    "    Used cobra_df = cobra_df[~pd.to_numeric(cobra_df[\"OccurTime\"],errors='coerce').isna()]  to check if all the values could be converted to numeric values and if not then the row was removed from cobra_df.\n",
    "    \n",
    "\n",
    "3. In the API Web Dataset there was a column for the state abbreviation and in the agency column there was also the state abbreviation infront of the agency number so I removed the unecessary state abbreviation as it was redundant. I use .str.replace() to replace the unecessary values with \"\"\n",
    "\n",
    "\n",
    "4. For the Downloaded dataset I renamed confusing and vague columns names like UCR Literal, Beat, and Location to more specific and descriptive names like CrimeType, PoliceBeat, and StreetAddress. I used .rename() to map the old column names to the new column names.\n",
    "\n",
    "\n",
    "5. I also had to drop multiple columns that had useless or unintersesting data, take uncessary additional words in data values, and sort the data to make more logical sense. I also had to convert numerous data columns to different types and create new columns to allow for more analysis in phase 3. Used .drop(),np.where(), and .astype() and .map()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjB_SbWY1LUB"
   },
   "source": [
    "## Jupyter Notebook Quick Tips\n",
    "Here are some quick formatting tips to get you started with Jupyter Notebooks. This is by no means exhaustive, and there are plenty of articles to highlight other things that can be done. We recommend using HTML syntax for Markdown but there is also Markdown syntax that is more streamlined and might be preferable. \n",
    "<a href = \"https://towardsdatascience.com/markdown-cells-jupyter-notebook-d3bea8416671\">Here's an article</a> that goes into more detail. (Double-click on cell to see syntax)\n",
    "\n",
    "# Heading 1\n",
    "## Heading 2\n",
    "### Heading 3\n",
    "#### Heading 4\n",
    "<br>\n",
    "<b>BoldText</b> or <i>ItalicText</i>\n",
    "<br> <br>\n",
    "Math Formulas: $x^2 + y^2 = 1$\n",
    "<br> <br>\n",
    "Line Breaks are done using br enclosed in < >.\n",
    "<br><br>\n",
    "Hyperlinks are done with: <a> https://www.google.com </a> or \n",
    "<a href=\"http://www.google.com\">Google</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb9oVjpRDswQ"
   },
   "source": [
    "# Data Collection and Cleaning\n",
    "You are required to provide data collection and cleaning for the three (3) minimum datasets. Create a function for each of the following sections that reads or scrapes data from a file or website, manipulate and cleans the parsed data, and writes the cleaned data into a new file. \n",
    "\n",
    "Make sure your data cleaning and manipulation process is not too simple. Performing complex manipulation and using modules not taught in class shows effort, which will increase the chance of receiving full credit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhaseII.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
